{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd360f2-c2a1-45aa-91f6-cb1c478a2b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:13:20.837080Z",
     "iopub.status.busy": "2025-04-27T21:13:20.837080Z",
     "iopub.status.idle": "2025-04-27T21:13:23.793881Z",
     "shell.execute_reply": "2025-04-27T21:13:23.793881Z",
     "shell.execute_reply.started": "2025-04-27T21:13:20.837080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Using cached captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from captum) (3.8.4)\n",
      "Requirement already satisfied: numpy<2.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from captum) (23.2)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from captum) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from captum) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (2023.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (68.2.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=1.10->captum) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from jinja2->torch>=1.10->captum) (2.1.3)\n",
      "Using cached captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94079415-b115-429a-b575-aff7a65b0084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:39.259990Z",
     "iopub.status.busy": "2025-04-27T21:14:39.258988Z",
     "iopub.status.idle": "2025-04-27T21:14:51.584976Z",
     "shell.execute_reply": "2025-04-27T21:14:51.584976Z",
     "shell.execute_reply.started": "2025-04-27T21:14:39.259990Z"
    },
    "id": "cfed6b27-e673-4f01-8a01-61339b6e1f63"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer, set_seed\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e04f3b5-bb5b-4161-ac81-c473be2eca33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:51.585979Z",
     "iopub.status.busy": "2025-04-27T21:14:51.585979Z",
     "iopub.status.idle": "2025-04-27T21:14:51.647056Z",
     "shell.execute_reply": "2025-04-27T21:14:51.647056Z",
     "shell.execute_reply.started": "2025-04-27T21:14:51.585979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random_seed = 42\n",
    "np_seed = 42\n",
    "torch_seed = 42\n",
    "transformers_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "np.random.seed(np_seed)\n",
    "torch.manual_seed(torch_seed)\n",
    "set_seed(transformers_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16454d2b-f061-423a-8e35-aa809864ce20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:51.648058Z",
     "iopub.status.busy": "2025-04-27T21:14:51.647056Z",
     "iopub.status.idle": "2025-04-27T21:14:51.650945Z",
     "shell.execute_reply": "2025-04-27T21:14:51.650945Z",
     "shell.execute_reply.started": "2025-04-27T21:14:51.648058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e099443e-9cae-4211-8f77-3701fd3202cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:51.651948Z",
     "iopub.status.busy": "2025-04-27T21:14:51.651948Z",
     "iopub.status.idle": "2025-04-27T21:14:52.204804Z",
     "shell.execute_reply": "2025-04-27T21:14:52.204804Z",
     "shell.execute_reply.started": "2025-04-27T21:14:51.651948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427f4930-7cac-4178-83ff-ea7c5ff9f6a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:52.205807Z",
     "iopub.status.busy": "2025-04-27T21:14:52.204804Z",
     "iopub.status.idle": "2025-04-27T21:14:57.257720Z",
     "shell.execute_reply": "2025-04-27T21:14:57.257720Z",
     "shell.execute_reply.started": "2025-04-27T21:14:52.205807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8afd43c07df4bb0b3d8695a68d98f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721ef971-160a-44ec-9efd-76510ada97e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:14:57.258722Z",
     "iopub.status.busy": "2025-04-27T21:14:57.257720Z",
     "iopub.status.idle": "2025-04-27T21:14:57.261661Z",
     "shell.execute_reply": "2025-04-27T21:14:57.261661Z",
     "shell.execute_reply.started": "2025-04-27T21:14:57.258722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5209f947-71c5-42cb-a401-b68058e6b789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:15:40.300266Z",
     "iopub.status.busy": "2025-04-27T21:15:40.300266Z",
     "iopub.status.idle": "2025-04-27T21:15:40.304860Z",
     "shell.execute_reply": "2025-04-27T21:15:40.304860Z",
     "shell.execute_reply.started": "2025-04-27T21:15:40.300266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a538d309-4a2c-4c1c-a746-5c30bf4197bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:19:18.183792Z",
     "iopub.status.busy": "2025-04-27T21:19:18.183792Z",
     "iopub.status.idle": "2025-04-27T21:19:18.186792Z",
     "shell.execute_reply": "2025-04-27T21:19:18.186792Z",
     "shell.execute_reply.started": "2025-04-27T21:19:18.183792Z"
    }
   },
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, LLMGradientAttribution\n",
    "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555afe07-3c29-40ef-a9ab-bfa33db1852f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:16:25.456553Z",
     "iopub.status.busy": "2025-04-27T21:16:25.456553Z",
     "iopub.status.idle": "2025-04-27T21:16:25.458713Z",
     "shell.execute_reply": "2025-04-27T21:16:25.458713Z",
     "shell.execute_reply.started": "2025-04-27T21:16:25.456553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "# input_text = \"Hey, are you conscious? Can you talk to me?\"\n",
    "input_text = \"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4ec841-794b-4c67-acd3-7da88e145a31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:19:38.184302Z",
     "iopub.status.busy": "2025-04-27T21:19:38.184302Z",
     "iopub.status.idle": "2025-04-27T21:19:38.186329Z",
     "shell.execute_reply": "2025-04-27T21:19:38.186329Z",
     "shell.execute_reply.started": "2025-04-27T21:19:38.184302Z"
    }
   },
   "outputs": [],
   "source": [
    "model_wrapper = ModelInputWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226132a7-6c43-4b59-8273-973d224bb249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:19:58.091350Z",
     "iopub.status.busy": "2025-04-27T21:19:58.091350Z",
     "iopub.status.idle": "2025-04-27T21:19:58.222542Z",
     "shell.execute_reply": "2025-04-27T21:19:58.222542Z",
     "shell.execute_reply.started": "2025-04-27T21:19:58.091350Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelInputWrapper' object has no attribute 'input_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lig \u001b[38;5;241m=\u001b[39m LayerIntegratedGradients(model_wrapper, model_wrapper\u001b[38;5;241m.\u001b[39minput_layer)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelInputWrapper' object has no attribute 'input_layer'"
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(model_wrapper, model_wrapper.input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e880101-11c1-41fc-9da8-89ac8e99c3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
