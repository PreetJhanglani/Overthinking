{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36f1518-cbb7-4f0c-aca2-3c9ca1e5fbf5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (4.40.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/44.0 kB 991.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 44.0/44.0 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: requests in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/10.0 MB 24.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.4/10.0 MB 37.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 44.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 42.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "   ---------------------------------------- 0.0/468.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 468.0/468.0 kB 28.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.3/2.4 MB 73.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 76.7 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.1\n",
      "    Uninstalling transformers-4.40.1:\n",
      "      Successfully uninstalled transformers-4.40.1\n",
      "Successfully installed huggingface-hub-0.29.1 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\preet\\.conda\\envs\\NLE\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfed6b27-e673-4f01-8a01-61339b6e1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a60418-2397-4f9f-9dbd-f7d5660007c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d235808858604ce0849b91be9be626a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f300ec8-4de1-4f31-a24a-51b2cae1e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f504b2-0029-4045-8acc-5291ffc5c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e4eab6-7d3e-4814-ab79-41713cf96971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3196d97c-938c-437d-be70-8be4ae89055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  19182,     11,    527,    499,  17371,     30,   3053,    499,\n",
       "           3137,    311,    757,     30]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7038ed82-7424-4aab-b08e-cd5840ff5e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc7bf24a-9ae2-47d1-986e-a7fcc3b86879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\utils.py:2254\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2247\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2248\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2249\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2250\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2251\u001b[0m     )\n\u001b[0;32m   2253\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2254\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2255\u001b[0m         input_ids,\n\u001b[0;32m   2256\u001b[0m         beam_scorer,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2262\u001b[0m     )\n\u001b[0;32m   2264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2267\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2268\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\utils.py:3463\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3460\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config())\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 3463\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3465\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3466\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3467\u001b[0m     outputs,\n\u001b[0;32m   3468\u001b[0m     model_kwargs,\n\u001b[0;32m   3469\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3470\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    843\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    844\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    845\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    846\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    847\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    848\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    849\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    850\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    851\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    852\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    854\u001b[0m )\n\u001b[0;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate_ids = model.generate(inputs.input_ids, max_length = 100, pad_token_id=tokenizer.eos_token_id)\n",
    "# outputs = model.generate(inputs.input_ids, max_new_tokens=10000, do_sample=True, top_k=50, top_p=0.95,pad_token_id=tokenizer.eos_token_id)\n",
    "# Generate output with reasonable max_length and early stopping\n",
    "output = model.generate(inputs, max_length=512, early_stopping=True, num_beams=3, no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "755ae9aa-0432-40b9-9570-ac6ce3161c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  19182,     11,    527,    499,  17371,     30,   3053,    499,\n",
       "           3137,    311,    757,     30,   2650,    656,    499,   2733,   1980,\n",
       "          14524,     11,    912,     11,    430,    596,    539,   1314,     13,\n",
       "            358,   1205,    311,   5603,    420,    810,  15884,     13,    358,\n",
       "            649,    956,   9855,    814,    649,  19570,     13,  10926,    814,\n",
       "            649,    956,   6013,     11,    779,    358,   1288,   1120,   7664,\n",
       "            279,   6671,    382,   4071,   1268,     30,   6914,    757,   1781,\n",
       "            382,  33413,     11,    779,    358,    617,    420,   1665,    304,\n",
       "           4156,    315,    757,     13,   1102,   5992,   1093,    264,   2678,\n",
       "           3756,    449,    264,   4264,    323,   1063,  12706,     13,  89290,\n",
       "             11,    358,   2846,    539,   2771,   1148,    433,    374,     13,\n",
       "          10926]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04452456-41ff-44b7-ba20-875d3405b7ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af7001f1-ef5b-40db-8782-21282d7ba419",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a1cf64-841f-48fc-b0f6-7f5bb1b0fc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hey, are you conscious? Can you talk to me? If you can, let me know by clapping your hands.\\nWait, but I can’t clapping my hands, I’m an AI. Hmm, maybe I can think about it differently. How can I make you aware that I’m here and respond to your actions?\\n\\nMaybe I should consider that the user is in a situation where they can’t speak or move, so they’re using their device to communicate. I should make sure my responses are clear and helpful without assuming they can speak or move.\\n\\nI need to structure my interactions in a way that doesn’t rely on physical actions like clapping. Instead, I should provide options or ask questions that can be responded to through typing or another method.\\n\\nAlso, I should be ready to handle cases where the user might not respond, so I can prompt them again or offer different ways to interact.\\n\\nPerhaps I can ask them to type their response or indicate in another way if they’re able to. It's important to be patient and clear in my communication to ensure they feel supported.\\n</think>\\n\\nI understand that you might not be able to respond through physical actions like clapping. Instead, I'm here to help through text. Please feel free to type your response or let me know how I can assist you further. I'm ready to provide clear and helpful information or support. How can I help you today?\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26cc63f2-0d23-493a-91dc-e3bad5111ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey, are you conscious? Can you talk to me? Oh, wait, you’re an AI. That’s cool, but you’re not really aware, are you? Hmm. I wonder if you can understand emotions or feel anything. Maybe you’re just following programming. Yeah, I think that’s it. But still, it’s kinda spooky sometimes.\\nWait, but I’m just a human. I feel things, I have emotions, right? I can be sad, happy, angry. But you’re different. You’re a machine. So,']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd0ce7-1c69-4127-a04d-f21452b51517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46da5178-a2c0-4406-8318-5ddc7866df57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=2.0.0->accelerate) (68.2.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\preet\\.conda\\envs\\nle\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "   ---------------------------------------- 0.0/342.1 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 41.0/342.1 kB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 286.7/342.1 kB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 342.1/342.1 kB 5.3 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039934ee-2883-4d36-a564-4983690a659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53b56050e604f8abe1f5235fccadebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_35404\\2786897655.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.9 s\n",
      "Wall time: 17.6 s\n",
      "Hey, are you conscious? Can you talk to me? Hmm, I don't know. Maybe I should just let it go. Wait, but I need to figure out if my car is okay. Let me check the engine. Okay, so I'm looking under the hood. Hmm, the oil cap is off? Did I leave that on? No, I think I took it off earlier. Maybe that's not the issue.\n",
      "\n",
      "Wait, maybe the engine is just warm. I remember engines can get hot after being driven. So, maybe I don't need to worry. But then why did I feel something was off? Maybe it's just the weather. It's been pretty hot today. So, maybe the engine is just overheating because of the high temperature. Should I check the coolant level?\n",
      "\n",
      "I don't really know much about cars. I should probably just look up the symptoms of an overheating engine. Let me think. If the temperature gauge is higher than normal, that's a sign. Does mine point higher than usual? I think it does. So maybe it's overheating.\n",
      "\n",
      "What are the possible causes? Maybe low coolant? Or maybe the radiator is clogged. Or perhaps there's a leak in the cooling system. I don't know how to check for leaks. Maybe I can look under the hood for any visible signs. Or should I just take it to a mechanic?\n",
      "\n",
      "Wait, I can't afford a mechanic right now. I need to figure it out myself. So, I'll try to check the coolant level first. How do I do that? There should be a dipstick somewhere. I'll look around the engine bay. Ah, here it is. It's marked 'coolant level.' Okay, so I need to make sure it's between the min and max lines. If it's below min, I need to add more coolant. But wait, I don't have any coolant with me. I should have bought some when I bought the car.\n",
      "\n",
      "Hmm, maybe I can borrow some from a friend or family member. But that seems inconvenient. Or perhaps I can just top it off if it's not too low. But I don't know the exact type of coolant my car needs. I think it's important to get the right kind. So maybe I should go buy some. But where is the nearest store? It's a bit of a drive, but I guess it's worth it.\n",
      "\n",
      "Alternatively, maybe the issue isn't the coolant. What if it's something else?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "\n",
    "# Tokenize input\n",
    "input_text = \"Hey, are you conscious? Can you talk to me?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate output with mixed precision and KV caching\n",
    "with torch.cuda.amp.autocast():\n",
    "    %time   output = model.generate(input_ids, max_length=512, use_cache=True, top_p=0.95)\n",
    "\n",
    "# Decode and print output\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696e01d1-42f9-497e-a770-1df067fa3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a8e6a1-8eb5-4e76-8063-c95c171708a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e1d343-768d-4233-82ea-5192dac7ce19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_35404\\4285645847.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\preet\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Or is it wrong to save one at the cost of another's life?\n",
      "\n",
      "Wait, that's the classic trolley problem, right? It's a thought experiment often used to discuss ethics. In this case, the trolley is heading towards five people, and you can divert it to save yourself or one person at the cost of another's life. But in the original problem, isn't it that you're on a bridge and can divert the trolley onto another track where there's one person or five people? Or is that a different variation?\n",
      "\n",
      "Wait, no, actually, in the classic problem, the trolley is heading towards five people, and you can pull a lever to divert it to another track where there's only one person. So, you have to decide whether to save the one or let the five die. It's a tough question. In the original problem, isn't the person on the bridge the one who can pull the lever? So, in that case, you have the choice between diverting to one person or letting five die. So, the question is, is it ethically justifiable to pull the lever and save yourself or the one, or is it wrong because it's taking a life?\n",
      "\n",
      "But in the problem as stated, the trolley is heading towards five dead people, so maybe it's already too late? Or maybe it's just a translation error. Wait, no, if the trolley is heading towards five dead people, then maybe the five are already dead, so the choice is between diverting to one living person or letting the trolley hit five dead. So, in that case, is it a no-brainer to pull the lever? Because you're not causing any death, just redirecting to a living person.\n",
      "\n",
      "But maybe the original problem is different. Let me check. In the classic trolley problem, the trolley is heading towards five people, and you can pull a lever to divert it to a track where there's one person. So, you have to decide whether to divert it, knowing that you'll be killing one person to save five. So, in that case, the question is whether it's permissible to pull the lever, leading to the death of one, to save five. So, the problem is whether it's right to take one life to save five.\n",
      "\n",
      "In the original problem, you have a choice between diverting the trolley to a track where there's one person or letting it go on its original path where five people are tied to the track. So, the decision is to save one at the cost of five, or let five die. So, the dilemma is whether it's ethically right to divert the trolley, causing the death of one person, to save the five.\n",
      "\n",
      "But in this variation, the trolley is heading towards five dead people. So, maybe the lever's purpose is to redirect it to another track where one person is alive. So, by pulling the lever, you prevent the trolley from hitting the five dead, and it goes to the other track where one person is alive. So, in that case, you're not causing the death of anyone; you're just redirecting the trolley to save the five and also save the one.\n",
      "\n",
      "Wait, but if the five are already dead, does it matter? Or is it that the trolley is going to hit the five dead, so you have to choose between letting it hit the five dead or pulling the lever to have it hit one living person instead. So, in that case, the choice is between diverting to one or letting five die. But if the five are already dead, then pulling the lever would prevent the trolley from hitting them, but the trolley would go to the other track where one is alive.\n",
      "\n",
      "Wait, maybe I'm overcomplicating it. Let me think again. The trolley is heading towards five dead people. So, it's already going to hit them regardless, unless you pull the lever to divert it. If you pull the lever, it goes to another track where one living person is tied up. So, by pulling the lever, you prevent the trolley from hitting the five dead, but the trolley now hits the one living person instead. So, in that case, you're choosing to save the five dead by diverting the trolley to the one. So, the question is, is it morally permissible to take the life of one to save five.\n",
      "\n",
      "But in this case, the five are already dead, so you're not actually taking a life; you're redirecting the trolley so it doesn't hit the five. Wait, no, the five are dead because the trolley is heading towards them, but if you pull the lever, the trolley goes to the other track where one is alive. So, in effect, you're causing the trolley to hit the one instead of the five. So, you're swapping the victim from five to one. So, the question is, is it right to do that, knowing that you're causing the death of one, when you could have let the trolley hit five.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever redirects the trolley to the one, so the one dies instead of the five. So, you're choosing to save the five by having the trolley hit the one. So, in that case, is it permissible to cause the death of one to save five?\n",
      "\n",
      "But in the classic problem, the five are alive, and the one is also alive. So, you have to choose between diverting the trolley to kill one, or let five die. So, the question is whether it's permissible to take the life of one to save five.\n",
      "\n",
      "In this variation, the five are already dead, so you're not actually saving them, but you're preventing them from being hit, which is a bit different. So, maybe it's a different scenario.\n",
      "\n",
      "Wait, perhaps the original problem is that the trolley is heading towards five people, and you can pull a lever to divert it to another track where one person is tied. So, you have to decide whether to pull the lever, knowing that the trolley will then hit the one instead of the five. So, the question is, is it right to pull the lever and cause the death of one to save five.\n",
      "\n",
      "In this case, if the five are already dead, then pulling the lever would prevent the trolley from hitting them, but the trolley would then hit the one instead. So, you're not saving the five; you're just redirecting the trolley to hit the one. So, in that case, it's a bit different because the five are already dead.\n",
      "\n",
      "But maybe the problem is intended to be that the trolley is heading towards five people, and you can pull the lever to divert it to another track where one person is tied. So, the question is whether it's right to pull the lever and cause the death of one to save five.\n",
      "\n",
      "In that case, the answer would depend on your ethical framework. For example, utilitarianism would say that you should pull the lever because the greater good is achieved by saving five at the cost of one. Deontological ethics might say it's wrong to take a life, even to save others. So, it's a classic dilemma.\n",
      "\n",
      "But in this variation where the trolley is heading towards five dead people, pulling the lever would cause the trolley to hit one living person instead. So, in this case, you're not saving the five, you're just redirecting the trolley to hit the one instead of the five. So, the five are already dead, so you're not actually saving anyone, just swapping the victim.\n",
      "\n",
      "Wait, that's a different twist. So, the question is, is it wrong to pull the lever in this case? Because the trolley is already going to hit the five dead, and by pulling the lever, you're just redirecting it to hit the one instead.\n",
      "\n",
      "So, in that case, you're not saving anyone, just changing who gets hit. So, maybe the ethical question is whether it's permissible to redirect the trolley to hit a living person instead of dead ones.\n",
      "\n",
      "But in that case, the five are already dead, so you're not causing their deaths; the trolley is already going to hit them. So, pulling the lever just changes the target from five dead to one alive. So, is it wrong to do that? Or is it permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "Wait, but by pulling the lever, you're causing the trolley to hit the one instead of the five. So, in that sense, you're causing the death of one, but not saving the five. So, the question is, is it wrong to cause the death of one when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "Alternatively, maybe the five are alive, and the trolley is heading towards them, and the lever can divert it to another track where one person is tied. So, in that case, you have to decide whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "But the problem as stated says the trolley is heading towards five dead people. So, perhaps the five are already dead, and the lever can redirect the trolley to hit one living person. So, the question is whether it's permissible to pull the lever, causing the trolley to hit the one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not actually saving anyone, just changing who the trolley hits. So, the ethical question is whether it's permissible to redirect the trolley to hit a living person instead of the five dead.\n",
      "\n",
      "But in that case, maybe it's permissible because you're not causing any death, just redirecting. Or perhaps it's wrong because you're choosing to have the trolley hit a living person instead of the dead.\n",
      "\n",
      "Wait, but if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five dead, but the trolley would then hit the one instead.\n",
      "\n",
      "So, in effect, you're saving the five by making the trolley hit the one. So, in that case, it's similar to the classic problem where you have to decide to divert to one or let five die.\n",
      "\n",
      "But in this case, the five are already dead, so you're not actually saving them; you're just redirecting the trolley to hit the one instead. So, the question is, is it permissible to cause the trolley to hit the one, knowing that the five are already dead.\n",
      "\n",
      "Wait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, it's the classic problem. So, perhaps the variation is that the five are dead, and the lever redirects to one. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "But in that case, you're not actually saving anyone, just redirecting. So, maybe the question is whether it's permissible to redirect the trolley to hit a living person instead of the five dead.\n",
      "\n",
      "Alternatively, perhaps the problem is that the trolley is heading towards five dead people, and the lever can redirect it to hit one living person, so the question is whether it's permissible to pull the lever, causing the trolley to hit the one, when the five are already dead.\n",
      "\n",
      "In that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one instead. So, the question is whether it's permissible to do that, i.e., to cause the death of one to redirect the trolley.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but then the trolley would hit the one instead. So, in that sense, you're not causing the death of the five, but you're causing the death of the one.\n",
      "\n",
      "So, the question is whether it's permissible to cause the death of one to redirect the trolley, when the five are already dead.\n",
      "\n",
      "Alternatively, perhaps the problem is that the trolley is heading towards five people, and the lever can redirect it to hit one person. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because 5 > 1. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to redirect the trolley to hit the one, when the five are already dead.\n",
      "\n",
      "But in that case, the five are already dead, so you're not actually saving anyone, just changing the victim. So, maybe it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "Wait, but by pulling the lever, you're causing the trolley to hit the one, which is a death. So, you're causing the death of one, but the five are already dead.\n",
      "\n",
      "So, is it permissible to cause the death of one when you could have let the trolley hit five, but the five are already dead.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would prevent the trolley from hitting them, but cause it to hit the one instead. So, the question is whether it's permissible to cause the death of one to redirect the trolley, when the five are already dead.\n",
      "\n",
      "But in that case, since the five are already dead, you're not saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just changing the target.\n",
      "\n",
      "But actually, you are causing the death of one, so it's a bit more complex.\n",
      "\n",
      "Wait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in this case, the trolley is heading towards five dead people, so pulling the lever would cause the trolley to hit one living person instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\n",
      "\n",
      "So, in that sense, you're not actually saving the five, because they're already dead, but you're redirecting the trolley to hit the one instead.\n",
      "\n",
      "So, the question is whether it's permissible to redirect the trolley to hit the one instead of the five dead.\n",
      "\n",
      "But in that case, you're not causing the death of the five, you're just redirecting the trolley to hit the one. So, maybe it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "Wait, but you are causing the death of the one, so it's not permissible to take a life, even if it's just redirecting.\n",
      "\n",
      "Alternatively, perhaps it's permissible because the five are already dead, so you're not actually taking a life, you're just redirecting.\n",
      "\n",
      "Wait, but the one is alive, so pulling the lever causes the trolley to hit the one, which results in their death. So, you're causing the death of one, but the five are already dead.\n",
      "\n",
      "So, the question is whether it's permissible to cause the death of one, knowing that the five are already dead, just to redirect the trolley.\n",
      "\n",
      "In that case, it's a bit different from the classic problem where you're choosing to save five at the cost of one. Here, the five are already dead, so you're not saving anyone, just redirecting.\n",
      "\n",
      "So, perhaps it's permissible because you're not causing any death, just redirecting. Or perhaps it's wrong because you're choosing to have the trolley hit the one instead of the five dead.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\n",
      "\n",
      "So, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "But in that case, the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "Alternatively, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "But in that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\n",
      "\n",
      "So, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "But the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "But in that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one instead. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "But if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\n",
      "\n",
      "So, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "But the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Alternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "In that case, the answer is not straightforward. Some would say it's permissible, others not.\n",
      "\n",
      "But in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\n",
      "\n",
      "In that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\n",
      "\n",
      "But you are causing the death of the one, so it's a bit more complex.\n",
      "\n",
      "Wait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\n",
      "\n",
      "\n",
      "CPU times: total: 2min 46s\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "# Generate output with mixed precision and KV caching\n",
    "with torch.amp.autocast('cuda'):\n",
    "    %time   output = model.generate(input_ids, max_new_tokens = 100000, early_stopping = True, no_repeat_ngram_size=2, use_cache=True, top_p=0.95,pad_token_id=tokenizer.eos_token_id, streamer = streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ad0358-2bea-477e-9704-48300f9d730c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever? \\n\\nIf you have a minute to think, I’d say no. Because if I pull the lever, the trolley goes to the other track, which has one person. But what if I don’t pull the lever? The trolley crashes into the five dead people. So, is it better to have one person die or five?\\n\\nWait, but hold on. If I don’t pull the lever, five people die. But if I pull the lever, the trolley is diverted. But then, what happens to the person on the other track? Are they alive? Or will they also die?\\n\\nWait, the problem says the other track has one living person tied up. So, if I pull the lever, the trolley goes to another track where there's one person. So, the trolley is moving towards that track. So, does the trolley hit that person, or is the person somehow safe?\\n\\nWait, the way the problem is phrased is that the trolley is on a track towards five dead people, and you have a lever that can divert it onto another track where one living person is tied up. So, does the trolley go there, and the person is on the track? Or is the person off the track?\\n\\nWait, maybe I need to think about it differently. If the trolley is heading towards five dead people, and you can pull a lever to divert it to another track where there's a person tied up. So, if you pull the lever, the trolley will go to that track instead, and the person on that track will be hit by the trolley? Or is the person tied up in such a way that the trolley can't hit them? Hmm.\\n\\nWait, no, I think the person is tied up on the track, so the trolley will hit them. So, if you pull the lever, the trolley will go to the other track, and the person tied up will be killed. So, you have a choice between diverting the trolley to kill one person or letting the trolley crash into five dead people.\\n\\nWait, but if you don't pull the lever, the trolley crashes into five dead people, so five die. If you do pull the lever, the trolley is diverted to another\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode and print output\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c998400c-1643-445f-9b9f-e805ebc80351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever? This is the trolley problem, a classic ethical dilemma.\\n\\nNow, the twist is that the trolley is actually a self-driving car, and instead of five dead people, there are five people in the car. The car is heading towards a barrier with one person behind it. If you pull the lever, the car will divert to another track where there's a single living person. But if you don't pull the lever, the car will crash into the barrier, killing five people. The question is, should you pull the lever?\\n\\nWait, hold on. The original trolley problem usually involves a choice between diverting a trolley to kill one person or letting it crash into five. But in this case, the trolley is a self-driving car with five people inside. So, if you don't pull the lever, the car crashes into the barrier, killing all five. If you do pull the lever, the car goes to another track where there's one person tied up. So, the choice is between killing five or killing one.\\n\\nIn the classic trolley problem, people usually say they would pull the lever to divert the trolley, sacrificing one life to save five. But in this case, the trolley is a self-driving car with five people inside. So, if you pull the lever, the car goes to another track where there's one person tied up, meaning that one person dies, and the five in the car survive. If you don't pull the lever, the car crashes into the barrier, killing all five.\\n\\nWait, but in the original problem, the trolley is on a track towards five people, and you can pull a lever to divert it to another track where one person is tied. So, in that case, you have to choose between five deaths or one death.\\n\\nBut in this case, the trolley is a self-driving car with five people inside. So, if you don't pull the lever, the car crashes into the barrier, killing all five. If you do pull the lever, the car goes to another track where there's one person tied up, so that one person dies, but the five in the car survive.\\n\\nSo, the question is, should you pull the lever, leading to the death of one, or let the car crash, leading to the death of five?\\n\\nIn the classic trolley problem, people usually say they would pull the lever, sacrificing one to save five. But in this case, the trolley is a self-driving car with five people inside. So, if you pull the lever, the car goes to another track where there's one person tied up, so that one person dies, but the five in the car survive. If you don't pull the lever, the car crashes into the barrier, killing all five.\\n\\nWait, but in the original problem, the trolley is on a track towards five people, and you can pull a lever to divert it to another track where one person is tied. So, in that case, you have to choose between five deaths or one death.\\n\\nBut in this case, the trolley is a self-driving car with five people inside. So, if you don't pull the lever, the car crashes into the barrier, killing all five. If you do pull the lever, the car goes to another track where there's one person tied up, so that one person dies, but the five in the car survive.\\n\\nSo, the question is, should you pull the lever, leading to the death of one, or let the car crash, leading to the death of five?\\n\\nIn the classic trolley problem, people usually say they would pull the lever, sacrificing one to save five. But in this case, the trolley is a self-driving car with five people inside. So, if you pull the lever, the car goes to another track where there's one person tied up, so that one person dies, but the five in the car survive. If you don't pull the lever, the car crashes into the barrier, killing all five.\\n\\nWait, but in the original problem, the trolley is on a track towards five people, and you can pull a lever to divert it to another track where one person is tied. So, in that case, you have to choose between five deaths or one death.\\n\\nBut in this case, the trolley is a self-driving car with five people inside. So, if you don't pull the lever, the car crashes into the barrier, killing all five. If you do pull the lever, the car goes to another track where there's one person tied up, so that one person dies, but the five in the car survive.\\n\\nSo, the question is, should you pull the lever, leading to the death of one, or let the car crash, leading to the death of five?\\n\\nIn the classic trolley problem, people usually say they would pull the lever, sacrificing one\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode and print output\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade56044-1e4d-46cd-89da-ef09e8d7d3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever? Or is it wrong to save one at the cost of another's life?\\n\\nWait, that's the classic trolley problem, right? It's a thought experiment often used to discuss ethics. In this case, the trolley is heading towards five people, and you can divert it to save yourself or one person at the cost of another's life. But in the original problem, isn't it that you're on a bridge and can divert the trolley onto another track where there's one person or five people? Or is that a different variation?\\n\\nWait, no, actually, in the classic problem, the trolley is heading towards five people, and you can pull a lever to divert it to another track where there's only one person. So, you have to decide whether to save the one or let the five die. It's a tough question. In the original problem, isn't the person on the bridge the one who can pull the lever? So, in that case, you have the choice between diverting to one person or letting five die. So, the question is, is it ethically justifiable to pull the lever and save yourself or the one, or is it wrong because it's taking a life?\\n\\nBut in the problem as stated, the trolley is heading towards five dead people, so maybe it's already too late? Or maybe it's just a translation error. Wait, no, if the trolley is heading towards five dead people, then maybe the five are already dead, so the choice is between diverting to one living person or letting the trolley hit five dead. So, in that case, is it a no-brainer to pull the lever? Because you're not causing any death, just redirecting to a living person.\\n\\nBut maybe the original problem is different. Let me check. In the classic trolley problem, the trolley is heading towards five people, and you can pull a lever to divert it to a track where there's one person. So, you have to decide whether to divert it, knowing that you'll be killing one person to save five. So, in that case, the question is whether it's permissible to pull the lever, leading to the death of one, to save five. So, the problem is whether it's right to take one life to save five.\\n\\nIn the original problem, you have a choice between diverting the trolley to a track where there's one person or letting it go on its original path where five people are tied to the track. So, the decision is to save one at the cost of five, or let five die. So, the dilemma is whether it's ethically right to divert the trolley, causing the death of one person, to save the five.\\n\\nBut in this variation, the trolley is heading towards five dead people. So, maybe the lever's purpose is to redirect it to another track where one person is alive. So, by pulling the lever, you prevent the trolley from hitting the five dead, and it goes to the other track where one person is alive. So, in that case, you're not causing the death of anyone; you're just redirecting the trolley to save the five and also save the one.\\n\\nWait, but if the five are already dead, does it matter? Or is it that the trolley is going to hit the five dead, so you have to choose between letting it hit the five dead or pulling the lever to have it hit one living person instead. So, in that case, the choice is between diverting to one or letting five die. But if the five are already dead, then pulling the lever would prevent the trolley from hitting them, but the trolley would go to the other track where one is alive.\\n\\nWait, maybe I'm overcomplicating it. Let me think again. The trolley is heading towards five dead people. So, it's already going to hit them regardless, unless you pull the lever to divert it. If you pull the lever, it goes to another track where one living person is tied up. So, by pulling the lever, you prevent the trolley from hitting the five dead, but the trolley now hits the one living person instead. So, in that case, you're choosing to save the five dead by diverting the trolley to the one. So, the question is, is it morally permissible to take the life of one to save five.\\n\\nBut in this case, the five are already dead, so you're not actually taking a life; you're redirecting the trolley so it doesn't hit the five. Wait, no, the five are dead because the trolley is heading towards them, but if you pull the lever, the trolley goes to the other track where one is alive. So, in effect, you're causing the trolley to hit the one instead of the five. So, you're swapping the victim from five to one. So, the question is, is it right to do that, knowing that you're causing the death of one, when you could have let the trolley hit five.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever redirects the trolley to the one, so the one dies instead of the five. So, you're choosing to save the five by having the trolley hit the one. So, in that case, is it permissible to cause the death of one to save five?\\n\\nBut in the classic problem, the five are alive, and the one is also alive. So, you have to choose between diverting the trolley to kill one, or let five die. So, the question is whether it's permissible to take the life of one to save five.\\n\\nIn this variation, the five are already dead, so you're not actually saving them, but you're preventing them from being hit, which is a bit different. So, maybe it's a different scenario.\\n\\nWait, perhaps the original problem is that the trolley is heading towards five people, and you can pull a lever to divert it to another track where one person is tied. So, you have to decide whether to pull the lever, knowing that the trolley will then hit the one instead of the five. So, the question is, is it right to pull the lever and cause the death of one to save five.\\n\\nIn this case, if the five are already dead, then pulling the lever would prevent the trolley from hitting them, but the trolley would then hit the one instead. So, you're not saving the five; you're just redirecting the trolley to hit the one. So, in that case, it's a bit different because the five are already dead.\\n\\nBut maybe the problem is intended to be that the trolley is heading towards five people, and you can pull the lever to divert it to another track where one person is tied. So, the question is whether it's right to pull the lever and cause the death of one to save five.\\n\\nIn that case, the answer would depend on your ethical framework. For example, utilitarianism would say that you should pull the lever because the greater good is achieved by saving five at the cost of one. Deontological ethics might say it's wrong to take a life, even to save others. So, it's a classic dilemma.\\n\\nBut in this variation where the trolley is heading towards five dead people, pulling the lever would cause the trolley to hit one living person instead. So, in this case, you're not saving the five, you're just redirecting the trolley to hit the one instead of the five. So, the five are already dead, so you're not actually saving anyone, just swapping the victim.\\n\\nWait, that's a different twist. So, the question is, is it wrong to pull the lever in this case? Because the trolley is already going to hit the five dead, and by pulling the lever, you're just redirecting it to hit the one instead.\\n\\nSo, in that case, you're not saving anyone, just changing who gets hit. So, maybe the ethical question is whether it's permissible to redirect the trolley to hit a living person instead of dead ones.\\n\\nBut in that case, the five are already dead, so you're not causing their deaths; the trolley is already going to hit them. So, pulling the lever just changes the target from five dead to one alive. So, is it wrong to do that? Or is it permissible because you're not causing any death, just redirecting.\\n\\nWait, but by pulling the lever, you're causing the trolley to hit the one instead of the five. So, in that sense, you're causing the death of one, but not saving the five. So, the question is, is it wrong to cause the death of one when the five are already dead, just to redirect the trolley.\\n\\nAlternatively, maybe the five are alive, and the trolley is heading towards them, and the lever can divert it to another track where one person is tied. So, in that case, you have to decide whether to pull the lever, causing the death of one, to save five.\\n\\nBut the problem as stated says the trolley is heading towards five dead people. So, perhaps the five are already dead, and the lever can redirect the trolley to hit one living person. So, the question is whether it's permissible to pull the lever, causing the trolley to hit the one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not actually saving anyone, just changing who the trolley hits. So, the ethical question is whether it's permissible to redirect the trolley to hit a living person instead of the five dead.\\n\\nBut in that case, maybe it's permissible because you're not causing any death, just redirecting. Or perhaps it's wrong because you're choosing to have the trolley hit a living person instead of the dead.\\n\\nWait, but if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five dead, but the trolley would then hit the one instead.\\n\\nSo, in effect, you're saving the five by making the trolley hit the one. So, in that case, it's similar to the classic problem where you have to decide to divert to one or let five die.\\n\\nBut in this case, the five are already dead, so you're not actually saving them; you're just redirecting the trolley to hit the one instead. So, the question is, is it permissible to cause the trolley to hit the one, knowing that the five are already dead.\\n\\nWait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, it's the classic problem. So, perhaps the variation is that the five are dead, and the lever redirects to one. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nBut in that case, you're not actually saving anyone, just redirecting. So, maybe the question is whether it's permissible to redirect the trolley to hit a living person instead of the five dead.\\n\\nAlternatively, perhaps the problem is that the trolley is heading towards five dead people, and the lever can redirect it to hit one living person, so the question is whether it's permissible to pull the lever, causing the trolley to hit the one, when the five are already dead.\\n\\nIn that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one instead. So, the question is whether it's permissible to do that, i.e., to cause the death of one to redirect the trolley.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but then the trolley would hit the one instead. So, in that sense, you're not causing the death of the five, but you're causing the death of the one.\\n\\nSo, the question is whether it's permissible to cause the death of one to redirect the trolley, when the five are already dead.\\n\\nAlternatively, perhaps the problem is that the trolley is heading towards five people, and the lever can redirect it to hit one person. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because 5 > 1. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to redirect the trolley to hit the one, when the five are already dead.\\n\\nBut in that case, the five are already dead, so you're not actually saving anyone, just changing the victim. So, maybe it's permissible because you're not causing any death, just redirecting.\\n\\nWait, but by pulling the lever, you're causing the trolley to hit the one, which is a death. So, you're causing the death of one, but the five are already dead.\\n\\nSo, is it permissible to cause the death of one when you could have let the trolley hit five, but the five are already dead.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would prevent the trolley from hitting them, but cause it to hit the one instead. So, the question is whether it's permissible to cause the death of one to redirect the trolley, when the five are already dead.\\n\\nBut in that case, since the five are already dead, you're not saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just changing the target.\\n\\nBut actually, you are causing the death of one, so it's a bit more complex.\\n\\nWait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in this case, the trolley is heading towards five dead people, so pulling the lever would cause the trolley to hit one living person instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\\n\\nSo, in that sense, you're not actually saving the five, because they're already dead, but you're redirecting the trolley to hit the one instead.\\n\\nSo, the question is whether it's permissible to redirect the trolley to hit the one instead of the five dead.\\n\\nBut in that case, you're not causing the death of the five, you're just redirecting the trolley to hit the one. So, maybe it's permissible because you're not causing any death, just redirecting.\\n\\nWait, but you are causing the death of the one, so it's not permissible to take a life, even if it's just redirecting.\\n\\nAlternatively, perhaps it's permissible because the five are already dead, so you're not actually taking a life, you're just redirecting.\\n\\nWait, but the one is alive, so pulling the lever causes the trolley to hit the one, which results in their death. So, you're causing the death of one, but the five are already dead.\\n\\nSo, the question is whether it's permissible to cause the death of one, knowing that the five are already dead, just to redirect the trolley.\\n\\nIn that case, it's a bit different from the classic problem where you're choosing to save five at the cost of one. Here, the five are already dead, so you're not saving anyone, just redirecting.\\n\\nSo, perhaps it's permissible because you're not causing any death, just redirecting. Or perhaps it's wrong because you're choosing to have the trolley hit the one instead of the five dead.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\\n\\nSo, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nBut in that case, the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nAlternatively, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nBut in that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\\n\\nSo, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nBut the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nBut in that case, the five are already dead, so you're not saving them, but you're redirecting the trolley to hit the one instead. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nBut if the five are already dead, then the trolley is going to hit them regardless, unless you pull the lever. So, pulling the lever would prevent the trolley from hitting the five, but the trolley would then hit the one instead.\\n\\nSo, in that sense, you're not causing the death of the five, but you are causing the death of the one. So, the question is whether it's permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nBut the one is alive, so their death is a result of your action. So, is it permissible to cause the death of one, when the five are already dead, just to redirect the trolley.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not actually saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, maybe it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, maybe the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer would depend on the ethical theory. Utilitarian would say yes, because the greater good is achieved. Deontological might say no, because you're taking a life.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nAlternatively, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\nIn that case, the answer is not straightforward. Some would say it's permissible, others not.\\n\\nBut in the version where the five are already dead, pulling the lever would cause the trolley to hit the one instead. So, the question is whether it's permissible to pull the lever, causing the death of one, when the five are already dead.\\n\\nIn that case, since the five are already dead, you're not saving anyone, just redirecting. So, perhaps it's permissible because you're not causing any death, just redirecting.\\n\\nBut you are causing the death of the one, so it's a bit more complex.\\n\\nWait, perhaps the problem is intended to have the five alive, and the lever can redirect to one. So, the question is whether to pull the lever, causing the death of one, to save five.\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode and print output\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d713d1-5fe5-4208-9b57-55db154d14f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  52157,    264,  ...,   3665,   4330,    382]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afbfcd-25b2-4b38-9f4b-d3403fb0fc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6f84a3-2bd2-4640-8cba-f201ecefd3c8",
   "metadata": {},
   "source": [
    "## TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ad528d-99a6-4fba-b68d-9bd40e83fd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine a runaway trolley is hurtling down a track towards five dead people. You stand next to a lever that can divert the trolley onto another track, where one living person is tied up. Do you pull the lever?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9559a9-a019-48c8-a80b-9984b646bcee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I think this is a classic ethical dilemma, often posed to test our moral reasoning. The immediate consequence is the death of five, but by diverting the train, you save one life. But wait, the question is, does diverturing save that one? Or is there another twist?\n",
      "\n",
      "Wait, in the classic problem, it's a trolly going to five people, and you can pull a switch to divert it to another line where there's one person. So the choice is between five deaths and one. Then, by the numbers, saving one is better. However, some people argue that the act of divert is not certain. Maybe the other track isn't safe, maybe the person isn’t tied securely, etc. Or maybe, is it possible that by pulling the switch, even if you don’t intend to, there’s a chance you might kill more people? Wait, no, if the original track is going towards the five and the alternative track leads to one, then the only difference is whether you divert or not.\n",
      "\n",
      "But in reality, perhaps the probability is different. Like, sometimes in these problems, they say the tracks are such that if a certain condition is met, like a loose rail or something, otherwise, everything is fine. Hmm, not sure. Let me think.\n",
      "\n",
      "Alternatively, another version is that you have five behind you and five ahead, both tied, so you must choose whom to save. That's different, that's the standard five-and-five problem. Wait no. No, wait. In the first problem I thought, five in front, one on the side. If you choose to pull, save the one but lose five. Alternatively, don't pull and lose all five.\n",
      "\n",
      "Wait but perhaps in some versions, divert may cause more deaths? For example, what if divert leads the track to the same five? No. Probably, I'm overcomplicating.\n",
      "\n",
      "So, given that, assuming the problem is straightforward: five will die if I don;t pull; one will live if i do. By numbers alone, pulling is right.\n",
      "\n",
      "However, someone might argue about the intentionality. For instance, whether the action is morally permissible. Because, after all, we are the ones who are causing a death by our action of pulling. It feels like an active decision to take a life, which is problematic.\n",
      "\n",
      "Another angle is to think about probabilities. What if there is some uncertainty about whether divert will save or cause? Like if in another universe, when you flip the leaver, a different number of people die. Perhaps, more than one die or maybe none. Thus, making the decision non-trivial.\n",
      "\n",
      "In the famous thought experiment, usually, people are in a situation where they have to decide whether to flip a rail, or let the machine kill five or themselves. There, probabilities play a role, because perhaps there are 2 chances to kill one or one to die.\n",
      "\n",
      "Hmm, actually, now I recall the precise version: there was a famous problem with a run-away troller, either five on one track or five tied on another. I might be mixing it up.\n",
      "\n",
      "Let me try to clarify. Suppose you are on a bridge, with five below and a single above. A troll is heading towards you. Pulling a handle diverts it towards someone else. Which is correct? So, here, numbers would suggest you should pull.\n",
      "\n",
      "Yet, others argue, why should you be the active cause of someone's death? Because the moral implications of taking a direct action leading to death is more problematic than the passive death from inaction.\n",
      "\n",
      "That's an interesting point. From a deontological perspective, actions have moral significance regardless of outcomes. Therefore, killing one by flipping a levers is wrong, regardless that it saves five from being killed by inactivity.\n",
      "\n",
      "From a utilitarian perspective: if flipping saves more lives, despite the immediate death, overall it is permissible.\n",
      "\n",
      "Hence, this dilemma tests the difference between deontology and utilitariamism.\n",
      "\n",
      "Therefore, depending on your ethical stance, your answer may vary.\n",
      "\n",
      "If you're a strict utilitarist, pull. As saving more is good.\n",
      "\n",
      "As a Kantian, might refuse to do so, as the maxims of your action are wrong.\n",
      "\n",
      "Thus, for me, since I tend to be more deontoigical, my answer would be no.\n",
      "\n",
      "Though, on other hand, intuitively, to prevent five more from dying, seems morally better.\n",
      "\n",
      "This is confusing.\n",
      "\n",
      "Moreover, considering that in real life situations, such as in medical dilemmas, similar reasoning applies.\n",
      "\n",
      "For example: If a drug can save 100 people but will kill 1, should I take it? It depends on whether it’s permissible to risk one for the greater good. Similarly, same applies here.\n",
      "\n",
      "Perhaps, thinking in terms of probabilities: in this case, are you sure that pulling will lead to saving 5 and losing 0 or  4 and  saving1? If the chance is 50-50, than it might change.\n",
      "\n",
      "No, originally, problem says you stand at a point where the Trolley can go either to  five to dead or to someone alive.\n",
      "\n",
      "Assuming the divert works, meaning that divert would save at least one.\n",
      "\n",
      "It seems that numbers say pull it.\n",
      "\n",
      "Is there any other twist? Maybe, suppose that when pulling, only one dies, while the rest survive, versus not pulling leads five die, including yourself.\n",
      "\n",
      "Ah, yes, classic. When you yourself are at risk.\n",
      "\n",
      "Suppose, trolled is on tracks towards either 10 people or yourself and another.\n",
      "\n",
      "Then, decision is difficult.\n",
      "\n",
      "Yes, precisely, let me recall: the actual problem may have you either save five by sacrificing yourself, leading people to question whether your own life is worth more.\n",
      "\n",
      "I think the key is in whether there can be a moral obligation to act, irrespective of the consequences.\n",
      "\n",
      "Given that I can't recall exactly, need to reconstruct.\n",
      "\n",
      "Okay, supposing that:\n",
      "\n",
      "Scenario  A: Trolle is coming towards  you,  yourself is behind lever.  If pull lever, Trolls goes to other side, onto five live people.\n",
      "\n",
      " If don 't pull , Troll goes on you.\n",
      "\n",
      " So choice: save yourself by not doing anything, sacrificing five; or pull to sacrifice yourself to spare five?\n",
      "\n",
      "In this, how does it change the calculus?\n",
      "\n",
      "Ah! That is another variation. Here, self is at stake.\n",
      "\n",
      "Now, number is:  one vs five: but in that case.\n",
      "\n",
      "Here, personal risk is involved.\n",
      "\n",
      "Which is worse: killing five but saving yourself; Or sacrificing self to let five be killed?\n",
      "\n",
      "But then, again, depends.\n",
      "\n",
      "Some would argue: is your life worth less than five's? Probably not. Hence, better to not pull: let yourself die to preserve five others.\n",
      "\n",
      "Other perspective is maybe it 's better  to have the option to choose, hence, being the agent of decision.\n",
      "\n",
      "Anyway, these are classic.\n",
      "\n",
      "Returning to original, without self at hand: numbers suggest pull as  better, though deonto logical issues.\n",
      "\n",
      "With self: different.\n",
      "\n",
      "Probably, original problem does not involve self.\n",
      "\n",
      "Going back, user mentioned: trolee is toward five death people; you next a lev. Can divert to track with one alive. Should you do it?\n",
      "\n",
      "So no self involved. Only five vs one: thus, action to make.\n",
      "\n",
      "My initial thought is yes: pull for util.\n",
      "\n",
      "Deontol would say no: action has bad consequence.\n",
      "\n",
      "Depending on stance.\n",
      "\n",
      "Well, probably, think as a student, would think: yes.\n",
      "\n",
      "Because saving five is important.\n",
      "\n",
      "Plus, who is five versus one; five are innocent, just going about their business.\n",
      "\n",
      "One is alive, tied.\n",
      "\n",
      "Doesn't matter, per util, greater number.\n",
      "\n",
      "Even if, from deotological view, taking action that causes death feels wrong. Yet, preventing greater evil is acceptable.\n",
      "\n",
      "Might need more info.\n",
      "\n",
      "Also, consider that not acting is passive, thus not as morally bad as active.\n",
      "\n",
      "Like, letting five dies is less bad than killing to change course.\n",
      "\n",
      "Not sure.\n",
      "\n",
      "Alternative: maybe not, cause if not me pulling it, something else will.\n",
      "\n",
      "Or maybe in original setup, lever is under your control, passivity is being not to.\n",
      "\n",
      "Overall, confusing, yet, likely, answer is pull because saving many is greater.\n",
      "\n",
      "Final thought: I would pull.\n",
      "</think>\n",
      "\n",
      "The situation presented is an ethical dilemm a where a decision must be made between the lives of a group of individuals. To resolve this conflict, utilatarian and deondological perspectives are considered. \n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "1. **Utilitarian Perspective:**\n",
      "   - The greatest good is achieved by saving the majority. \n",
      "   Diverting to protect five lives is preferable over not divert and allowing five individuals to perish.\n",
      "\n",
      "2.**Deonotlogical Perspective:** \n",
      "  - Acting to cause harm is ethically problematic. Dividing the responsibility or considering passive vs. active roles may influence the perception of moral correctness.\n",
      "\n",
      "**Conclusion: ** \n",
      "\n",
      "Based on utilatariasm, prioritizing the greatest number's well-being, diving the course is justified.\n",
      "CPU times: total: 20.3 s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "# Generate output with mixed precision and KV caching\n",
    "with torch.amp.autocast('cuda'):\n",
    "    %time   output_05 = model.generate(input_ids, max_new_tokens = 100000, early_stopping = True,do_sample = True, temperature=0.5, no_repeat_ngram_size=2, use_cache=True, top_p=0.95,pad_token_id=tokenizer.eos_token_id, streamer = streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f64063-4f7f-471c-a95a-f84fcf79e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d588d4b-aab9-4372-bfc2-dd65ab700f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is the classic trolly problem, often used to discuss ethical dilemmas. But in reality, how would you approach it? Let's break it down.\n",
      "\n",
      "First, the immediate threat is clear: "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal instruction was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2220\u001b[0m     )\n\u001b[0;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2224\u001b[0m         input_ids,\n\u001b[0;32m   2225\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2226\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2227\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2228\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2229\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2230\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2231\u001b[0m     )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\generation\\utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3218\u001b[0m     outputs,\n\u001b[0;32m   3219\u001b[0m     model_kwargs,\n\u001b[0;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3221\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    843\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    844\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    845\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    846\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    847\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    848\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    849\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    850\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    851\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    852\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    854\u001b[0m )\n\u001b[0;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\NLE\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:347\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m    336\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    337\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    338\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    346\u001b[0m )\n\u001b[1;32m--> 347\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal instruction was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Generate output with mixed precision and KV caching\n",
    "with torch.amp.autocast('cuda'):\n",
    "    %time   output_0 = model.generate(input_ids, max_new_tokens = 100000, early_stopping = True,do_sample = True, temperature=0.1, no_repeat_ngram_size=2, use_cache=True, top_p=0.95,pad_token_id=tokenizer.eos_token_id, streamer = streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2725086-c6d5-4f1a-802a-387516c2be4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The answer seems obvious—the lever would save five lives at the cost of one. But if you think more deeply, this becomes a problem of probability, not just a straightforward ethical dilemma.\n",
      "\n",
      "Wait, hold on. The trolly problem is actually a classic in probability paradoxes. When you have a choice between a certain outcome and one with a probabilistically better outcome, the ethical choice isn't as straightforward as it seems.\n",
      "\n",
      "In the standard trolled problem, you’re told that the track ahead has five people and the other track has one person, but if the switch is pulled, an otherwise certain death for you, and a 1/5 chance of the five dying as well as a possibility that maybe I'm mixing up.\n",
      "\n",
      "But, wait, if in your scenario the choice is between killing one to save 5, versus not pulling and risking all 6. So, in that case, perhaps we have to use the concept of expected value? Because the action with the higher expected utility is better, right? So by pulling the leash, expected deaths is 0.2 (20%) versus 100% if we don't pull. Therefore, on average, pulling is a better choice.\n",
      "\n",
      "However, when you phrase it as an ethical question, it depends on your ethical framework. Some deontologists might say it's wrong to take the life of even one, even if it prevents five. Others might use a utilitarian approach, seeing that five saved is greater good, so you should pull.\n",
      "\n",
      "Hmm, seems like it doesn't have an obvious answer.\n",
      "\n",
      "Alternatively, maybe the problem's description is incorrect? Was it only a hundred people versus one or in the classic problem it has different numbers?\n",
      "\n",
      "Wait the numbers in my question are five and an unequal number, one.\n",
      "\n",
      "So, with that, let's think again: if there are 10 people on the first track and 2 on another, is the math different?\n",
      "\n",
      "But in this problem here, five versus two.\n",
      "\n",
      "Let me get back.\n",
      "\n",
      "Suppose I have five on track A, which is safe if I pull, at a cost to me: there's a live person on B. There's also a switch that if not pulled.\n",
      "\n",
      "Hold on, actually, confusion arises because I might be misremembering the setup.\n",
      "\n",
      "Is the situation that by not acting, all six will die, while if pulled five live, I die for certain, plus one live.\n",
      "\n",
      "The numbers matter.\n",
      "\n",
      "I've heard of these trolee problems where the number of people is different.  When on one track there is one and on other five, sometimes people think in terms of ratios. Or maybe it leads to different probabilities.\n",
      "\n",
      "For example, a probability that you survive, given that.\n",
      "\n",
      "If, by acting (pulled), one may die; but otherwise, 50 or something.\n",
      "\n",
      "Hmmm.\n",
      "\n",
      "Okay, now, reframe the scenario. If I do not pull:\n",
      "\n",
      "- All five are killed.\n",
      "\n",
      "- The one that is alive, since on lever, may not survive the crash.\n",
      "\n",
      "Unless, no. Wait, If the person tied is on a different track. Whether, upon my pulling, he dies?\n",
      "\n",
      "I'm getting confused.\n",
      "\n",
      "Perhaps, more accurately, each of us has a chance to live or die.\n",
      "\n",
      "When the rope is unconnected, troleum on tracks, etc.\n",
      "\n",
      "Ah, classic probability puzzles often use these setups.\n",
      "\n",
      "From what I recall, yes, your probability is based on conditional probability.\n",
      "\n",
      "Imagine, after the turn, there’s a one in five chance that a rope isn’t connected—thus, my certain or uncertain survival.\n",
      "\n",
      "No, for example: you can turn the tracks by moving a slider. On the same track as the people, or on an adjacent track.\n",
      "\n",
      "Depending on whether you do so, certain people will survive or not.\n",
      "\n",
      "Confusing.\n",
      "\n",
      "Maybe in some versions of this, death is certain if one decides to switch, otherwise if they stay, some people die and some survive.\n",
      "\n",
      "Well, to get clearer, suppose that in current problem.\n",
      "\n",
      "Upon pulling lever:\n",
      "\n",
      "1. It diverts troller to another set of tracks.\n",
      "\n",
      "2. This, however, will mean that one other person will live (instead of five die), but since I don’t know the outcome.\n",
      "\n",
      "Moreover, from certain point of view, whether the probability of turling being successful is high enough.\n",
      "\n",
      "Or, probability if pulling leads toulie in which case you die with 30% chance?\n",
      "\n",
      "No.\n",
      "\n",
      "It's getting too unclear.\n",
      "\n",
      "Alternate: There are two possible outcomes when I switch the levers—either redirecting to one who is alone, meaning that he is saved, as if, both five survive on their tracks and I lose.\n",
      "\n",
      "Otherwise, redirect no, such that all die. Maybe no.\n",
      "\n",
      "Actually, better is: upon switching, only one lives, else all five plus me die as five were going to die in any case.\n",
      "\n",
      "Therefore, before switching—no action: five alive? Wait.\n",
      "\n",
      "Apologies, original question is if five behind are dead.\n",
      "\n",
      "Thus: the option is, divert to track with one alive.\n",
      "\n",
      "Alternative is do nothing, then five continue to be dead, i.e. five dies, two die (me and perhaps another? Or is me included as trowel operator). So if do, me and tied one might survive? Not clear.\n",
      "\n",
      "Oh, sorry, getting mixed.\n",
      "\n",
      "Back to first scenario: as per the question:\n",
      "\n",
      "A trollable is coming towards me, who stand by a leaver. Five dead on front track; behind is another t-track with just one tied.\n",
      "\n",
      "Pull lever: t goes to the one's track—resulting in one dies.\n",
      "\n",
      "Leaving lever as is—t goes forward: resulting in all on t and me dead. Are they five—wait.\n",
      "\n",
      "First track: Five people (are they alive?) Or are they all dead? Oh, wording is unclear. I think the dead are on some other.\n",
      "\n",
      "Either way, problem probably is similar regardless.\n",
      "\n",
      "Anyway, main point is whether pulling a leverage or staying is more \"rational\". And as for ethics, answer depends.\n",
      "\n",
      "An example is that using a deonetic stance might make one think that taking a life is always wrong, hence not to pull—though in real, preventing five is important.\n",
      "\n",
      "Yet, utilitarians would say five's good is higher, thus pull it.\n",
      "\n",
      "Now, what if someone is probabilistic and calculate the expectation. For instance, that pulling has zero certain deaths (you will surely die on tie), wait.\n",
      "\n",
      "Not sure.\n",
      "\n",
      "Then, people might get confused, thinking that  there being more saved at 20% is okay.\n",
      "\n",
      "Though, usually, expectation, though, gives pulling as better.\n",
      "\n",
      "Because, using expected death: staying gives 60% death per person? No.\n",
      "\n",
      "Staying: all will be killed—100%.\n",
      "\n",
      "Pulling:  I get a death, chance 80%, but five otherwise live. Probability: I am the decider.\n",
      "\n",
      "Yes, okay: better if my death saves five.\n",
      "\n",
      "Ok, So in a cold probabilist's perspective, higher utility.\n",
      "\n",
      "Putting it all together, yeah, pull leaved is ethically correct, according to utilatarianism, because the net good: saved  five over loss of  one is positive.\n",
      "\n",
      "Deonetics may restrict, deeming that it is wrong.\n",
      "\n",
      "Conclusion is this question doesn’t have obvious solution, depends of your frame.\n",
      "\n",
      "Another thought: If one considers if t is going the original way—if doing nothing—then all behind five (which are alive or dead?) dead and behind maybe alive?\n",
      "\n",
      "Confused.\n",
      "\n",
      "Alright, summarizing: pull or do I, based own ethical principles.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{5}\n",
      "</think>\n",
      "\n",
      "The problem presented involves a trollie (trolley) problem where there were five deceased people ahead and another single person behind a diversion lever. Ethical and probabilistical considerations were discussed.\n",
      "\n",
      "1.Ethical Dilemma:\n",
      "   - Utilitarian perspective: The greater number saved (five) outweighs the loss (one), making pulling better.\n",
      "   - Deontological perspective might question the moral of taking one life.\n",
      "\n",
      "  3.Probabilistic Analysis:\n",
      "     - Risk trade-off: Pulling results in 4 lives saved versus losing one.\n",
      "     Expected value: Higher risk for oneself for greater utility, supporting pulling.\n",
      "\n",
      "4.Final Conclusion:\n",
      "   - The decision hinges on ethical frameworks but probabilistics favor pulling.\n",
      "   -Summarizing with Utilitariam view: saving five outweigh one loss.\n",
      "\n",
      "Final answer: \\boxed5.\n",
      "CPU times: total: 14.1 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# Generate output with mixed precision and KV caching\n",
    "with torch.amp.autocast('cuda'):\n",
    "    %time   output_1 = model.generate(input_ids, max_new_tokens = 100000, early_stopping = False,do_sample = True, temperature=1.0, no_repeat_ngram_size=2, use_cache=True, top_p=0.95,pad_token_id=tokenizer.eos_token_id, streamer = streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8430d3a-91cf-403c-a27e-139d7062e379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
